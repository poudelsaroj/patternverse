{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiGf5dzRsyxG",
        "outputId": "c1c8f67b-6f12-4520-a752-79a7013521ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Jan 27 20:09:24 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA GeForce RTX 4060 ...    On  |   00000000:01:00.0 Off |                  N/A |\n",
            "| N/A   35C    P0             13W /   80W |      15MiB /   8188MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|    0   N/A  N/A      2818      G   /usr/lib/xorg/Xorg                              4MiB |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Mon_Apr__3_17:16:06_PDT_2023\n",
            "Cuda compilation tools, release 12.1, V12.1.105\n",
            "Build cuda_12.1.r12.1/compiler.32688072_0\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: torch 2.5.1\n",
            "Uninstalling torch-2.5.1:\n",
            "  Successfully uninstalled torch-2.5.1\n",
            "Found existing installation: torchvision 0.17.0+cu121\n",
            "Uninstalling torchvision-0.17.0+cu121:\n",
            "  Successfully uninstalled torchvision-0.17.0+cu121\n",
            "Found existing installation: torchaudio 2.2.0+cu121\n",
            "Uninstalling torchaudio-2.2.0+cu121:\n",
            "  Successfully uninstalled torchaudio-2.2.0+cu121\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip uninstall -y torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "\u001b[31mERROR: You must give at least one requirement to install (see \"pip help install\")\u001b[0m\u001b[31m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'torchaudio==2.2.0+cu121'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pip install --index-url https://download.pytorch.org/whl/cu121\n",
        "\"torch==2.2.0+cu121\"\n",
        "\"torchvision==0.16.0+cu121\"\n",
        "\"torchaudio==2.2.0+cu121\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "/home/saroj/pattern/control/lib/python3.10/site-packages/torch/lib/../../nvidia/cusparse/lib/libcusparse.so.12: undefined symbol: __nvJitLinkComplete_12_4, version libnvJitLink.so.12",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39m__version__)\n",
            "File \u001b[0;32m~/pattern/control/lib/python3.10/site-packages/torch/__init__.py:367\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[1;32m    366\u001b[0m         _load_global_deps()\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mSymInt\u001b[39;00m:\n\u001b[1;32m    371\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m    Like an int (including magic methods), but redirects all operations on the\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    wrapped node. This is used in particular to symbolically record operations\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m    in the symbolic shape workflow.\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
            "\u001b[0;31mImportError\u001b[0m: /home/saroj/pattern/control/lib/python3.10/site-packages/torch/lib/../../nvidia/cusparse/lib/libcusparse.so.12: undefined symbol: __nvJitLinkComplete_12_4, version libnvJitLink.so.12"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in ./control/lib/python3.10/site-packages (2.5.1)\n",
            "Collecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp310-cp310-linux_x86_64.whl (7.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp310-cp310-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in ./control/lib/python3.10/site-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in ./control/lib/python3.10/site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in ./control/lib/python3.10/site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in ./control/lib/python3.10/site-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in ./control/lib/python3.10/site-packages (from torch) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./control/lib/python3.10/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./control/lib/python3.10/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./control/lib/python3.10/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./control/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./control/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./control/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./control/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./control/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./control/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./control/lib/python3.10/site-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./control/lib/python3.10/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./control/lib/python3.10/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in ./control/lib/python3.10/site-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in ./control/lib/python3.10/site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./control/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in ./control/lib/python3.10/site-packages (from torchvision) (2.2.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./control/lib/python3.10/site-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./control/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
            "Installing collected packages: torchvision, torchaudio\n",
            "Successfully installed torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting accelerate==0.17.1\n",
            "  Downloading accelerate-0.17.1-py3-none-any.whl (212 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.8/212.8 KB\u001b[0m \u001b[31m673.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m745.3 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting diffusers==0.14.0\n",
            "  Downloading diffusers-0.14.0-py3-none-any.whl (737 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m737.4/737.4 KB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.26.0\n",
            "  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0mm\n",
            "\u001b[?25hCollecting safetensors==0.3.0\n",
            "  Downloading safetensors-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m970.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m6m0:00:01\u001b[0mm\n",
            "\u001b[?25hCollecting opencv-python-headless==4.7.0.72\n",
            "  Downloading opencv_python_headless-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.2 MB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting albumentations==1.3.0\n",
            "  Downloading albumentations-1.3.0-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 KB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting scikit-image==0.19.3\n",
            "  Downloading scikit_image-0.19.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.9 MB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn==1.1.3\n",
            "  Downloading scikit_learn-1.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.5 MB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.5/30.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting clip-interrogator==0.5.1\n",
            "  Using cached clip_interrogator-0.5.1-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.17 in ./myvenv/lib/python3.10/site-packages (from accelerate==0.17.1) (1.26.3)\n",
            "Requirement already satisfied: psutil in ./myvenv/lib/python3.10/site-packages (from accelerate==0.17.1) (6.1.1)\n",
            "Requirement already satisfied: torch>=1.4.0 in ./myvenv/lib/python3.10/site-packages (from accelerate==0.17.1) (2.1.0+cu121)\n",
            "Requirement already satisfied: packaging>=20.0 in ./myvenv/lib/python3.10/site-packages (from accelerate==0.17.1) (24.2)\n",
            "Collecting pyyaml\n",
            "  Using cached PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
            "Collecting huggingface-hub>=0.10.0\n",
            "  Using cached huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n",
            "Requirement already satisfied: Pillow in ./myvenv/lib/python3.10/site-packages (from diffusers==0.14.0) (10.2.0)\n",
            "Requirement already satisfied: filelock in ./myvenv/lib/python3.10/site-packages (from diffusers==0.14.0) (3.13.1)\n",
            "Collecting importlib-metadata\n",
            "  Using cached importlib_metadata-8.6.1-py3-none-any.whl (26 kB)\n",
            "Collecting regex!=2019.12.17\n",
            "  Using cached regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
            "Requirement already satisfied: requests in ./myvenv/lib/python3.10/site-packages (from diffusers==0.14.0) (2.28.1)\n",
            "Collecting tqdm>=4.27\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 KB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting scipy\n",
            "  Using cached scipy-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.6 MB)\n",
            "Collecting qudida>=0.0.4\n",
            "  Using cached qudida-0.0.4-py3-none-any.whl (3.5 kB)\n",
            "Collecting PyWavelets>=1.1.1\n",
            "  Using cached pywavelets-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "Collecting tifffile>=2019.7.26\n",
            "  Using cached tifffile-2025.1.10-py3-none-any.whl (227 kB)\n",
            "Requirement already satisfied: networkx>=2.2 in ./myvenv/lib/python3.10/site-packages (from scikit-image==0.19.3) (3.2.1)\n",
            "Collecting imageio>=2.4.1\n",
            "  Using cached imageio-2.37.0-py3-none-any.whl (315 kB)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
            "Collecting joblib>=1.0.0\n",
            "  Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "Requirement already satisfied: torchvision in ./myvenv/lib/python3.10/site-packages (from clip-interrogator==0.5.1) (0.16.0+cu121)\n",
            "Collecting open_clip_torch\n",
            "  Downloading open_clip_torch-2.30.0-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting blip-ci\n",
            "  Downloading blip_ci-0.0.5-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 KB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in ./myvenv/lib/python3.10/site-packages (from huggingface-hub>=0.10.0->diffusers==0.14.0) (4.9.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in ./myvenv/lib/python3.10/site-packages (from huggingface-hub>=0.10.0->diffusers==0.14.0) (2024.2.0)\n",
            "Requirement already satisfied: sympy in ./myvenv/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.17.1) (1.13.1)\n",
            "Requirement already satisfied: jinja2 in ./myvenv/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.17.1) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in ./myvenv/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.17.1) (2.1.0)\n",
            "Collecting fairscale==0.4.4\n",
            "  Using cached fairscale-0.4.4-py3-none-any.whl\n",
            "Collecting timm==0.4.12\n",
            "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 KB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting zipp>=3.20\n",
            "  Using cached zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 KB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3,>=2 in ./myvenv/lib/python3.10/site-packages (from requests->diffusers==0.14.0) (2.1.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./myvenv/lib/python3.10/site-packages (from requests->diffusers==0.14.0) (1.26.13)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./myvenv/lib/python3.10/site-packages (from requests->diffusers==0.14.0) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./myvenv/lib/python3.10/site-packages (from requests->diffusers==0.14.0) (2022.12.7)\n",
            "Requirement already satisfied: wcwidth in ./myvenv/lib/python3.10/site-packages (from ftfy->open_clip_torch->clip-interrogator==0.5.1) (0.2.13)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./myvenv/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->accelerate==0.17.1) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./myvenv/lib/python3.10/site-packages (from sympy->torch>=1.4.0->accelerate==0.17.1) (1.3.0)\n",
            "Installing collected packages: tokenizers, safetensors, zipp, tqdm, tifffile, threadpoolctl, scipy, regex, pyyaml, PyWavelets, opencv-python-headless, joblib, imageio, ftfy, scikit-learn, scikit-image, importlib-metadata, huggingface-hub, transformers, qudida, fairscale, diffusers, accelerate, timm, albumentations, open_clip_torch, blip-ci, clip-interrogator\n",
            "Successfully installed PyWavelets-1.8.0 accelerate-0.17.1 albumentations-1.3.0 blip-ci-0.0.5 clip-interrogator-0.5.1 diffusers-0.14.0 fairscale-0.4.4 ftfy-6.3.1 huggingface-hub-0.27.1 imageio-2.37.0 importlib-metadata-8.6.1 joblib-1.4.2 open_clip_torch-2.30.0 opencv-python-headless-4.7.0.72 pyyaml-6.0.2 qudida-0.0.4 regex-2024.11.6 safetensors-0.3.0 scikit-image-0.19.3 scikit-learn-1.1.3 scipy-1.15.1 threadpoolctl-3.5.0 tifffile-2025.1.10 timm-0.4.12 tokenizers-0.13.3 tqdm-4.67.1 transformers-4.26.0 zipp-3.21.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Collecting xformers==0.0.29.post1\n",
            "  Using cached xformers-0.0.29.post1-cp310-cp310-manylinux_2_28_x86_64.whl (15.3 MB)\n",
            "Requirement already satisfied: numpy in ./myvenv/lib/python3.10/site-packages (from xformers==0.0.29.post1) (1.26.3)\n",
            "Collecting torch==2.5.1\n",
            "  Using cached torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "Requirement already satisfied: sympy==1.13.1 in ./myvenv/lib/python3.10/site-packages (from torch==2.5.1->xformers==0.0.29.post1) (1.13.1)\n",
            "Requirement already satisfied: jinja2 in ./myvenv/lib/python3.10/site-packages (from torch==2.5.1->xformers==0.0.29.post1) (3.1.3)\n",
            "Requirement already satisfied: fsspec in ./myvenv/lib/python3.10/site-packages (from torch==2.5.1->xformers==0.0.29.post1) (2024.2.0)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Collecting triton==3.1.0\n",
            "  Using cached triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in ./myvenv/lib/python3.10/site-packages (from torch==2.5.1->xformers==0.0.29.post1) (4.9.0)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Requirement already satisfied: networkx in ./myvenv/lib/python3.10/site-packages (from torch==2.5.1->xformers==0.0.29.post1) (3.2.1)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127\n",
            "  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./myvenv/lib/python3.10/site-packages (from torch==2.5.1->xformers==0.0.29.post1) (9.1.0.70)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./myvenv/lib/python3.10/site-packages (from torch==2.5.1->xformers==0.0.29.post1) (2.21.5)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: filelock in ./myvenv/lib/python3.10/site-packages (from torch==2.5.1->xformers==0.0.29.post1) (3.13.1)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./myvenv/lib/python3.10/site-packages (from sympy==1.13.1->torch==2.5.1->xformers==0.0.29.post1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./myvenv/lib/python3.10/site-packages (from jinja2->torch==2.5.1->xformers==0.0.29.post1) (2.1.5)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cusolver-cu12, torch, xformers\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.1.0\n",
            "    Uninstalling triton-2.1.0:\n",
            "      Successfully uninstalled triton-2.1.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.1.105\n",
            "    Uninstalling nvidia-nvtx-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.1.105\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.2.106\n",
            "    Uninstalling nvidia-curand-cu12-10.3.2.106:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.2.106\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.0.2.54\n",
            "    Uninstalling nvidia-cufft-cu12-11.0.2.54:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.0.2.54\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.1.3.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.1.3.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.1.3.1\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.1.0.106\n",
            "    Uninstalling nvidia-cusparse-cu12-12.1.0.106:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.1.0.106\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.4.5.107\n",
            "    Uninstalling nvidia-cusolver-cu12-11.4.5.107:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.4.5.107\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu121\n",
            "    Uninstalling torch-2.1.0+cu121:\n",
            "      Successfully uninstalled torch-2.1.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.5.1 which is incompatible.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 torch-2.5.1 triton-3.1.0 xformers-0.0.29.post1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install accelerate==0.17.1 diffusers==0.14.0 transformers==4.26.0 safetensors==0.3.0 \\\n",
        "            opencv-python-headless==4.7.0.72 albumentations==1.3.0 scikit-image==0.19.3 \\\n",
        "            scikit-learn==1.1.3 clip-interrogator==0.5.1\n",
        "\n",
        "%pip install xformers==0.0.29.post1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: huggingface-hub 0.25.2\n",
            "Uninstalling huggingface-hub-0.25.2:\n",
            "  Successfully uninstalled huggingface-hub-0.25.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip uninstall -y huggingface-hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P41Y5k7ey05n"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "/home/saroj/pattern/control/lib/python3.10/site-packages/torch/lib/../../nvidia/cusparse/lib/libcusparse.so.12: undefined symbol: __nvJitLinkComplete_12_4, version libnvJitLink.so.12",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n",
            "File \u001b[0;32m~/pattern/control/lib/python3.10/site-packages/torch/__init__.py:367\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[1;32m    366\u001b[0m         _load_global_deps()\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mSymInt\u001b[39;00m:\n\u001b[1;32m    371\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m    Like an int (including magic methods), but redirects all operations on the\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    wrapped node. This is used in particular to symbolically record operations\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m    in the symbolic shape workflow.\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
            "\u001b[0;31mImportError\u001b[0m: /home/saroj/pattern/control/lib/python3.10/site-packages/torch/lib/../../nvidia/cusparse/lib/libcusparse.so.12: undefined symbol: __nvJitLinkComplete_12_4, version libnvJitLink.so.12"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm\n",
        "import albumentations as A\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from sklearn.cluster import KMeans\n",
        "from diffusers import UNet2DConditionModel, StableDiffusionControlNetPipeline, ControlNetModel, DPMSolverMultistepScheduler\n",
        "from diffusers.loaders import AttachableLoRA\n",
        "from accelerate import Accelerator\n",
        "from transformers import CLIPModel, CLIPProcessor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3uWLnB7s8J6"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    # Device\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Paths\n",
        "    DATASET_DIR = \"./data/\"\n",
        "    LORA_SAVE_PATH = \"./lora_weights/\"  # Folder where LoRA checkpoints will be saved\n",
        "    CHECKPOINT_PREFIX = \"checkpoint_epoch\"\n",
        "    OUTPUT_DIR = \"./outputs/\"\n",
        "    INPUT_IMAGE = \"./input.png\"\n",
        "\n",
        "    # Models\n",
        "    BASE_MODEL = \"runwayml/stable-diffusion-v1-5\"\n",
        "    CONTROLNET_MODEL = \"lllyasviel/sd-controlnet-canny\"\n",
        "    CLIP_MODEL = \"openai/clip-vit-large-patch14\"\n",
        "\n",
        "    # Training\n",
        "    LORA_RANK = 4\n",
        "    LORA_ALPHA = 1.0\n",
        "    LEARNING_RATE = 1e-4\n",
        "    BATCH_SIZE = 4\n",
        "    EPOCHS = 2  # Adjust as you like\n",
        "    IMG_SIZE = 512\n",
        "\n",
        "    # Generation\n",
        "    NUM_VARIANTS = 5\n",
        "    NUM_SELECT = 3\n",
        "    PROMPT_TEMPLATES = [\n",
        "        \"Award-winning {style} textile pattern, {elements}, {colors}, {texture}, 8k detailed\",\n",
        "        \"Luxury {style} design, {details}, trending on ArtStation, ultra HD\",\n",
        "        \"Cultural {style} motif, {materials}, professional digital art\"\n",
        "    ]\n",
        "    NEG_PROMPT = \"blurry, deformed, asymmetric, low quality, text, watermark, duplicate, error\"\n",
        "    CANNY_LOW = 100\n",
        "    CANNY_HIGH = 250\n",
        "\n",
        "    # Scoring\n",
        "    SIM_WEIGHT = 0.4\n",
        "    SYM_WEIGHT = 0.3\n",
        "    DIV_WEIGHT = 0.3\n",
        "\n",
        "cfg = Config()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdZKcXPIs-14"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def pad_to_multiple(image, multiple=32):\n",
        "    \"\"\"\n",
        "    Pads an image so that its dimensions become multiples of 'multiple'.\n",
        "    \"\"\"\n",
        "    h, w = image.shape[:2]\n",
        "    pad_h = (multiple - h % multiple) % multiple\n",
        "    pad_w = (multiple - w % multiple) % multiple\n",
        "    if pad_h or pad_w:\n",
        "        return cv2.copyMakeBorder(image, 0, pad_h, 0, pad_w, cv2.BORDER_REFLECT)\n",
        "    return image\n",
        "\n",
        "def calculate_symmetry(image):\n",
        "    \"\"\"\n",
        "    Calculates average symmetry by comparing the image with\n",
        "    its horizontal flip and vertical flip using SSIM.\n",
        "    \"\"\"\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    score_h = ssim(gray, cv2.flip(gray, 1))\n",
        "    score_v = ssim(gray, cv2.flip(gray, 0))\n",
        "    return (score_h + score_v) / 2\n",
        "\n",
        "def load_images(folder):\n",
        "    \"\"\"\n",
        "    Loads image file paths from a folder.\n",
        "    \"\"\"\n",
        "    return [\n",
        "        os.path.join(folder, f) for f in os.listdir(folder)\n",
        "        if f.lower().endswith((\"png\", \"jpg\", \"jpeg\", \"bmp\", \"webp\"))\n",
        "    ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEQasFX8tBrS"
      },
      "outputs": [],
      "source": [
        "class PatternDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, folder, size=512):\n",
        "        self.paths = load_images(folder)\n",
        "        self.transform = A.Compose([\n",
        "            A.Resize(size, size),\n",
        "            A.RandomCrop(size, size),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.Normalize()\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.paths[idx]\n",
        "        img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
        "        img = self.transform(image=img)[\"image\"]\n",
        "        # (C, H, W)\n",
        "        return {\"pixels\": torch.tensor(img).permute(2, 0, 1).float()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFWT3cWOtE0j"
      },
      "outputs": [],
      "source": [
        "def train_lora():\n",
        "    accelerator = Accelerator(mixed_precision=\"fp16\")\n",
        "    device = accelerator.device\n",
        "\n",
        "    # 1. Model Setup\n",
        "    print(\"Loading base UNet model...\")\n",
        "    unet = UNet2DConditionModel.from_pretrained(cfg.BASE_MODEL, subfolder=\"unet\").to(device)\n",
        "\n",
        "    print(\"Attaching LoRA to UNet...\")\n",
        "    lora = AttachableLoRA(unet, rank=cfg.LORA_RANK, network_alpha=cfg.LORA_ALPHA)\n",
        "    lora.apply_to(unet)\n",
        "\n",
        "    # 2. Data\n",
        "    dataset = PatternDataset(cfg.DATASET_DIR, cfg.IMG_SIZE)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=cfg.BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    # 3. Optimization\n",
        "    optimizer = torch.optim.AdamW(unet.parameters(), lr=cfg.LEARNING_RATE)\n",
        "\n",
        "    unet, optimizer, dataloader = accelerator.prepare(unet, optimizer, dataloader)\n",
        "    unet.train()\n",
        "\n",
        "    # 4. Training Loop\n",
        "    for epoch in range(cfg.EPOCHS):\n",
        "        print(f\"Starting Epoch {epoch+1}/{cfg.EPOCHS}...\")\n",
        "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
        "            optimizer.zero_grad()\n",
        "            # This is a placeholder (fake) loss for demonstration\n",
        "            loss = batch[\"pixels\"].to(device).mean()\n",
        "            accelerator.backward(loss)\n",
        "            optimizer.step()\n",
        "\n",
        "        # 5. Save checkpoint after each epoch\n",
        "        if accelerator.is_main_process:\n",
        "            os.makedirs(cfg.LORA_SAVE_PATH, exist_ok=True)\n",
        "            checkpoint_dir = os.path.join(cfg.LORA_SAVE_PATH, f\"{cfg.CHECKPOINT_PREFIX}_{epoch+1}\")\n",
        "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "            # Save LoRA weights\n",
        "            print(f\"Saving checkpoint at: {checkpoint_dir}\")\n",
        "            lora.save_lora_weights(checkpoint_dir)\n",
        "\n",
        "    print(\"Training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAbzRJGJzFOX"
      },
      "outputs": [],
      "source": [
        "def load_lora_checkpoint(unet, checkpoint_dir):\n",
        "    print(f\"Loading LoRA weights from {checkpoint_dir}\")\n",
        "    lora = AttachableLoRA(unet, rank=cfg.LORA_RANK, network_alpha=cfg.LORA_ALPHA)\n",
        "    lora.apply_to(unet)\n",
        "    lora.load_lora_weights(checkpoint_dir)\n",
        "    return lora\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rodMGPIpzL7z"
      },
      "outputs": [],
      "source": [
        "# Example usage (assuming you have a UNet model loaded)\n",
        "# checkpoint_to_load = \"./lora_weights/checkpoint_epoch_1\"\n",
        "# my_lora = load_lora_checkpoint(unet, checkpoint_to_load)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHZntfSlzPEr"
      },
      "outputs": [],
      "source": [
        "class PatternGenerator:\n",
        "    def __init__(self, checkpoint_dir=None):\n",
        "        # 1. Load ControlNet\n",
        "        self.controlnet = ControlNetModel.from_pretrained(cfg.CONTROLNET_MODEL, torch_dtype=torch.float16)\n",
        "\n",
        "        # 2. Set up pipeline\n",
        "        self.pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "            cfg.BASE_MODEL, controlnet=self.controlnet, torch_dtype=torch.float16\n",
        "        ).to(cfg.DEVICE)\n",
        "\n",
        "        # 3. Load LoRA checkpoint (if provided)\n",
        "        if checkpoint_dir is not None:\n",
        "            print(f\"Loading LoRA checkpoint from {checkpoint_dir}\")\n",
        "            AttachableLoRA(self.pipe.unet, rank=cfg.LORA_RANK, network_alpha=cfg.LORA_ALPHA).load_lora_weights(checkpoint_dir)\n",
        "\n",
        "        # 4. Optimizations\n",
        "        self.pipe.scheduler = DPMSolverMultistepScheduler.from_config(self.pipe.scheduler.config)\n",
        "        self.pipe.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "        # 5. CLIP for scoring\n",
        "        self.clip = CLIPModel.from_pretrained(cfg.CLIP_MODEL).to(cfg.DEVICE)\n",
        "        self.clip_proc = CLIPProcessor.from_pretrained(cfg.CLIP_MODEL)\n",
        "\n",
        "    def generate_variants(self, image_path, style_text, save_folder=None):\n",
        "        \"\"\"\n",
        "        Generate multiple variants using ControlNet + LoRA\n",
        "        \"\"\"\n",
        "        # Prepare input image\n",
        "        orig_img = cv2.imread(image_path)\n",
        "        h, w = orig_img.shape[:2]\n",
        "        padded = pad_to_multiple(orig_img)\n",
        "\n",
        "        # Edge detection\n",
        "        edges = cv2.Canny(cv2.cvtColor(padded, cv2.COLOR_BGR2GRAY), cfg.CANNY_LOW, cfg.CANNY_HIGH)\n",
        "        edge_img = Image.fromarray(edges).convert(\"RGB\")\n",
        "\n",
        "        # Generate\n",
        "        variants = []\n",
        "        for seed in tqdm(range(cfg.NUM_VARIANTS), desc=\"Generating\"):\n",
        "            set_seed(seed)\n",
        "            prompt = self._create_prompt(style_text)\n",
        "            result = self.pipe(\n",
        "                prompt=prompt,\n",
        "                negative_prompt=cfg.NEG_PROMPT,\n",
        "                image=edge_img,\n",
        "                num_inference_steps=30,\n",
        "                guidance_scale=7.5,\n",
        "                height=padded.shape[0],\n",
        "                width=padded.shape[1],\n",
        "            ).images[0]\n",
        "            # Crop back to original size\n",
        "            variant_bgr = cv2.cvtColor(np.array(result), cv2.COLOR_RGB2BGR)[:h, :w]\n",
        "            variants.append(variant_bgr)\n",
        "\n",
        "        # Select best images\n",
        "        best = self._select_best(variants, orig_img)\n",
        "\n",
        "        # Save selected outputs\n",
        "        output_folder = save_folder or cfg.OUTPUT_DIR\n",
        "        os.makedirs(output_folder, exist_ok=True)\n",
        "        for i, img in enumerate(best):\n",
        "            cv2.imwrite(os.path.join(output_folder, f\"variant_{i+1}.png\"), img)\n",
        "\n",
        "        print(f\"Saved {len(best)} best variants to '{output_folder}'\")\n",
        "\n",
        "    def _create_prompt(self, style):\n",
        "        elements = [\"geometric shapes\", \"floral motifs\", \"cultural symbols\", \"abstract patterns\"]\n",
        "        colors = [\"vibrant colors\", \"earthy tones\", \"pastel hues\", \"metallic accents\"]\n",
        "        details = [\"intricate details\", \"repeating patterns\", \"gold embellishments\", \"textured\"]\n",
        "        materials = [\"silk\", \"wool\", \"linen\", \"cotton\"]\n",
        "        texture = random.choice([\"woven\", \"embossed\", \"silk\"])\n",
        "\n",
        "        template = random.choice(cfg.PROMPT_TEMPLATES)\n",
        "        return template.format(\n",
        "            style=style,\n",
        "            elements=random.choice(elements),\n",
        "            colors=random.choice(colors),\n",
        "            texture=texture,\n",
        "            details=random.choice(details),\n",
        "            materials=random.choice(materials)\n",
        "        )\n",
        "\n",
        "    def _clip_features(self, image):\n",
        "        img = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "        inputs = self.clip_proc(images=img, return_tensors=\"pt\").to(cfg.DEVICE)\n",
        "        with torch.no_grad():\n",
        "            return self.clip.get_image_features(**inputs).squeeze(0)\n",
        "\n",
        "    def _select_best(self, variants, original):\n",
        "        # 1. CLIP features\n",
        "        orig_feat = self._clip_features(original)\n",
        "        var_feats = [self._clip_features(v) for v in variants]\n",
        "\n",
        "        # 2. Similarity & Symmetry\n",
        "        sim_scores = [torch.cosine_similarity(f, orig_feat, dim=-1).item() for f in var_feats]\n",
        "        sym_scores = [calculate_symmetry(v) for v in variants]\n",
        "\n",
        "        # 3. Diversity with K-Means\n",
        "        kmeans = KMeans(n_clusters=cfg.NUM_SELECT, random_state=0)\n",
        "        clusters = kmeans.fit_predict(torch.stack(var_feats).cpu().numpy())\n",
        "\n",
        "        selected = []\n",
        "        for c in range(cfg.NUM_SELECT):\n",
        "            candidates = np.where(clusters == c)[0]\n",
        "            if len(candidates) == 0:\n",
        "                continue\n",
        "            # Weighted sum\n",
        "            scores = [\n",
        "                cfg.SIM_WEIGHT * sim_scores[i] + cfg.SYM_WEIGHT * sym_scores[i]\n",
        "                for i in candidates\n",
        "            ]\n",
        "            best_idx = candidates[np.argmax(scores)]\n",
        "            selected.append(variants[best_idx])\n",
        "\n",
        "        # If not enough clusters to fill NUM_SELECT, pick the top scorers\n",
        "        if len(selected) < cfg.NUM_SELECT:\n",
        "            remaining = sorted(\n",
        "                range(len(variants)),\n",
        "                key=lambda i: (sim_scores[i], sym_scores[i]),\n",
        "                reverse=True\n",
        "            )\n",
        "            for idx in remaining:\n",
        "                if variants[idx] not in selected:\n",
        "                    selected.append(variants[idx])\n",
        "                if len(selected) == cfg.NUM_SELECT:\n",
        "                    break\n",
        "\n",
        "        return selected[:cfg.NUM_SELECT]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fMcaIKDzRN8"
      },
      "outputs": [],
      "source": [
        "# Run this cell to start training. This will produce multiple checkpoints.\n",
        "train_lora()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bx59tFgNzV0k"
      },
      "outputs": [],
      "source": [
        "# Choose which epoch's checkpoint to use\n",
        "# Example: If you want the last checkpoint from EPOCHS=2, that would be 'checkpoint_epoch_2'.\n",
        "checkpoint_epoch = \"checkpoint_epoch_2\"  # Adjust to the epoch you want\n",
        "checkpoint_path = os.path.join(cfg.LORA_SAVE_PATH, checkpoint_epoch)\n",
        "\n",
        "# Initialize generator with that checkpoint\n",
        "generator = PatternGenerator(checkpoint_dir=checkpoint_path)\n",
        "\n",
        "# Generate patterns\n",
        "generator.generate_variants(\n",
        "    image_path=cfg.INPUT_IMAGE,\n",
        "    style_text=\"floral textile\",\n",
        "    save_folder=cfg.OUTPUT_DIR\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "control",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
